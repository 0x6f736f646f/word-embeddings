{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas  as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora,models,similarities\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "import  difflib\n",
    "import nltk.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Data/posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0x6f736f646f\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\0x6f736f646f\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for i in range(df.__len__()):\n",
    "    questions=str(df['body'][i]).lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocess=[]\n",
    "    preprocess.append(nltk.tokenize.word_tokenize(questions))\n",
    "    filtered_sentence = \" \".join([w for w in preprocess[0] if not w in stop_words])\n",
    "    tokens = nltk.wordpunct_tokenize(filtered_sentence)\n",
    "    text = nltk.Text(tokens)\n",
    "    words = \" \".join([w.lower() for w in text if w.isalpha()])\n",
    "    df['body'][i]=words\n",
    "    \n",
    "    questions=str(df['comments'][i]).lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preprocess=[]\n",
    "    preprocess.append(nltk.tokenize.word_tokenize(questions))\n",
    "    filtered_sentence = \" \".join([w for w in preprocess[0] if not w in stop_words])\n",
    "    tokens = nltk.wordpunct_tokenize(filtered_sentence)\n",
    "    text = nltk.Text(tokens)\n",
    "    words = \" \".join([w.lower() for w in text if w.isalpha()])\n",
    "    df['comments'][i]=words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Data/preprocess.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0x6f736f646f\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "C:\\Users\\0x6f736f646f\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "texts = df.to_dict('records')\n",
    "documents = [TaggedDocument(text['body'].split(), [text['comments']])  for text in texts]   ## Pass the 'questions' & 'answers'\n",
    "\n",
    "model = gensim.models.Doc2Vec(vector_size=100, window=2, min_count=1, workers=11,alpha=0.025, min_alpha=0.025, epochs=20)\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, epochs=model.iter, total_examples=model.corpus_count)\n",
    "\n",
    "model.save(\"../doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QABot:\n",
    "    def __init__(self, data, model):\n",
    "        self.df = data\n",
    "        self.model = model\n",
    "        \n",
    "    def avg_feature_vector(self, sentence, model, num_features, index2word_set):\n",
    "        #FUNCTION TO AVERAGE  ALL  OUR FEATURE  VECTORS  IN EVERY DOCUMENT \n",
    "        words = sentence.split()\n",
    "        feature_vec = np.zeros((num_features,), dtype='float32') #Initialize empty array\n",
    "        n_words = 0\n",
    "        for word in words:\n",
    "            if word in index2word_set:\n",
    "                n_words += 1\n",
    "                feature_vec = np.add(feature_vec, model[word])\n",
    "        if (n_words > 0):\n",
    "            feature_vec = np.divide(feature_vec, n_words)\n",
    "        return feature_vec\n",
    "\n",
    "    def cosine_dist(self, user_asked):\n",
    "        #COSINE  SIMILARITY  BETWEEN  VECTORS  AND  GIVE PERCENTAGE  OF MATCH  BETWEEN  COMMON QUESTIONS  \n",
    "\n",
    "        index2word_set = set(self.model.wv.index2word)\n",
    "        try:\n",
    "            all_ratios = []\n",
    "            questn_df = pd.DataFrame({})\n",
    "            for i in range(self.df.__len__()):\n",
    "                s1_afv = self.avg_feature_vector(user_asked, model=self.model, num_features=100,\n",
    "                                                 index2word_set=index2word_set)\n",
    "                s2_afv = self.avg_feature_vector(self.df['questions'][i], model=self.model, num_features=100,\n",
    "                                                 index2word_set=index2word_set)\n",
    "                all_ratios.append(1 - spatial.distance.cosine(s1_afv, s2_afv))\n",
    "            questn_df = pd.DataFrame({\"questions\": list(self.df['questions']), \"answers\": list(self.df['answers']),\n",
    "                                      \"type\": list(self.df['type']),\n",
    "                                      \"ratios\": all_ratios})\n",
    "            final_ratio = questn_df.sort_values('ratios', ascending=False)\n",
    "\n",
    "            if final_ratio.empty:\n",
    "\n",
    "                return 'sorry  didnt  understand  your question'\n",
    "            else:\n",
    "\n",
    "                return final_ratio.head(5)\n",
    "        except:\n",
    "            return 'sorry  didnt  understand  your question'\n",
    "\n",
    "    def preprocedd_user_inpt(self, user_asked):\n",
    "        #THIS FUNCTION IS USED  TO  PREPROCESS  OUR  USER INPUT BY REMOVING  STOPWORDS  LIKE \"IS\" ,\"AND\" ,\"THE\" , TOKENIZE  THE  USER INPUT .\n",
    "        a = user_asked.lower()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        preprocess = []\n",
    "        preprocess.append(nltk.tokenize.word_tokenize(a))  ### tokenize  user input\n",
    "        filtered_sentence = \" \".join([w for w in preprocess[0] if not w in stop_words])  ##  remove stopwords\n",
    "        words = filtered_sentence\n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    data = pd.read_csv(\"../Data/preprocess.csv\")\n",
    "    model = Doc2Vec.load(\"../doc2vec.model\")\n",
    "    data['body'].fillna(\"nothing\", inplace=True)\n",
    "    Bot = QABot(data, model)\n",
    "    print('write your question below : ')  #### initial  message  shown to users\n",
    "    user_ask = input()\n",
    "    user_inpt = int(input('Press 1 to know the answer to your question  | Press 2 to know top 5 probabable  matching answers'))\n",
    "\n",
    "    if (user_inpt == 1):  ######### If user input  ==1  ,  it will  show  you  the answer to the question\n",
    "        userIp = Bot.preprocedd_user_inpt(int(user_ask))  ######## Calling the  preprocesser  function from class Bot\n",
    "        predict_question = Bot.cosine_dist(user_ask).iloc[0].answers  #### Calling the cosine_dist function from class Bot\n",
    "        print(predict_question)\n",
    "    elif (user_inpt == 2):  ######### If user input  ==2  ,  it will  show  you  the top 5  matching  questions with probabilities .\n",
    "        userIp = Bot.preprocedd_user_inpt(user_ask)  ######## Calling the  preprocesser  function from class Bot\n",
    "        predict_question = Bot.cosine_dist(int(user_ask))[\n",
    "            ['questions', 'answers', 'ratios']]  #### Calling the cosine_dist function from class Bot\n",
    "        print(predict_question)\n",
    "\n",
    "    else:\n",
    "        print('please type 1 or 2 for answer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write your question below : \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Hello\n",
      "Press 1 to know the answer to your question  | Press 2 to know top 5 probabable  matching answers 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please type 1 or 2 for answer\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
